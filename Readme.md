


# Model-Fusion Paper: 

This GitHub repository summarizes most recent papers and resources related to the model fusion.

If you have any suggestions about this repository, please feel free to start a new issue or pull requests.

### Weighted Average

- Fisher-Mering: Merging Models with Fisher-Weighted Averaging [[Paper]](https://arxiv.org/abs/2111.09832) 
- [ICLR23] RegMean: Dataless Knowledge Fusion by Merging Weights of Language Models [[Paper]](https://arxiv.org/abs/2212.09849) [[Code]](https://github.com/bloomberg/dataless-model-merging)
- [ICML22] Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time [[Paper]](https://arxiv.org/abs/2203.05482) [[Code]](https://github.com/mlfoundations/model-soups)

### LCM / Weight Permuting 

- [ICLR23] Git Re-Basin: Merging Models modulo Permutation Symmetries [[Paper]](https://arxiv.org/pdf/2209.04836) 
- [ICLR24] ZipIt! Merging Models from Different Tasks without Training [[Paper]](https://arxiv.org/abs/2305.03053) [[Code]](https://github.com/gstoica27/ZipIt)
- REPAIR: REnormalizing Permuted Activations for Interpolation Repair [[Paper]](https://arxiv.org/pdf/2211.08403) [[Code]](https://github.com/KellerJordan/REPAIR)

### Task-Arithmetic-based:

- [ICLR23] Editing Models with Task Arithmetic [[Paper]](https://arxiv.org/abs/2212.04089v3) [[Code]](https://github.com/gortizji/tangent_task_arithmetic)
- [NIPS23] Composing Parameter-Efficient Modules with Arithmetic Operations [[Paper]](https://arxiv.org/pdf/2306.14870) [[Code]](https://github.com/hkust-nlp/PEM_composition)
- [NIPS23] Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models [[Paper]](https://arxiv.org/abs/2305.12827) [[Code]](https://github.com/gortizji/tangent_task_arithmetic)
- [ICLR24] Parameter Efficient Multi-task Model Fusion with Partial Linearization [[Paper]](https://arxiv.org/abs/2310.04742)
- [ICLR24] AdaMerging: Adaptive Model Merging for Multi-Task Learning [[Paper]](https://arxiv.org/abs/2310.02575) [[Code]](https://github.com/EnnengYang/AdaMerging)
- MAP: Low-compute Model Merging with Amortized Pareto Fronts via Quadratic Approximation [[Paper]](https://arxiv.org/abs/2406.07529)

### Sign 

- [NIPS23] TIES-MERGING: Resolving Interference When Merging Models [[Paper]](https://arxiv.org/pdf/2306.01708) [[Code]](https://github.com/prateeky2806/ties-merging)


### Mask-based

- [ICML24] DARE: Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch [[Paper]](https://arxiv.org/abs/2311.03099) [[Code]](https://github.com/yule-BUAA/MergeLM)
- [ICML24] Representation Surgery for Multi-Task Model Merging [[Paper]](https://arxiv.org/pdf/2402.02705) [[Code]](https://github.com/EnnengYang/RepresentationSurgery)
- [ICML24] Localizing Task Information for Improved Model Merging and Compression [[Paper]](https://arxiv.org/abs/2405.07813) [[Code]](https://github.com/nik-dim/tall_masks)
- EMR-Merging: Tuning-Free High-Performance Model Merging [[Paper]](https://arxiv.org/pdf/2405.17461) [[Code]](https://github.com/harveyhuang18/EMR_Merging)

## Hetogorous Architecture

- LLM Augmented LLMs: Expanding Capabilities through Composition [[Paper]](https://arxiv.org/abs/2401.02412) [[Code]](https://github.com/lucidrains/CALM-pytorch)


### Learning-based Fusion

- [ICLR24] FOE: Fusing Models with Complementary Expertise [[Paper]](http://arxiv.org/abs/2310.01542) [[Code]](https://github.com/hwang595/FoE-ICLR2024)
- [ICLR24] FuseLLM: KNOWLEDGE FUSION OF LARGE LANGUAGE MODELS [[Paper]](https://arxiv.org/pdf/2401.10491) [[Code]](https://github.com/fanqiwan/FuseAI) 
- [ICML24] Merging Multi-Task Models via Weight-Ensembling Mixture of Experts [[Paper]](https://arxiv.org/abs/2310.02575) [[Code]](https://github.com/EnnengYang/AdaMerging)
- [ICML24] Learning to Route Among Specialized Experts for Zero-Shot Generalization  [[Paper]](https://arxiv.org/pdf/2402.05859) [[Code]](https://github.com/r-three/phatgoose)
- Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM [[Paper]](https://arxiv.org/pdf/2403.07816) 

### Test-Time Fusion

- Pack of LLMs: Model Fusion at Test-Time via Perplexity Optimization [[Paper]](https://arxiv.org/pdf/2404.11531) [[Code]](https://github.com/cmavro/PackLLM)


## Multi-Task Learning 

- [NIPS23] ForkMerge: Mitigating Negative Transfer in Auxiliary-Task Learning [[Paper]](https://arxiv.org/abs/2301.12618)
